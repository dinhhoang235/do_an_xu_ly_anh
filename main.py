"""
License Plate Recognition System - Using Hybrid KNN Model
Best performance: 57.81% accuracy on foreign plates
"""

import cv2
import os
import sys
import argparse
import time
import pickle
from pathlib import Path
import numpy as np
import pandas as pd

# ThÃªm path Ä‘á»ƒ import cÃ¡c module
sys.path.append(str(Path(__file__).parent / "src"))

from src.character_recognizer import CharacterRecognizer
from src.preprocessor import Preprocessor
from src.plate_detector import PlateDetector
from src.skew_corrector import SkewCorrector

class LicensePlateSystem:
    def __init__(self):
        self.preprocessor = Preprocessor()
        self.char_recognizer = CharacterRecognizer()
        self.plate_detector = PlateDetector()
        self.skew_corrector = SkewCorrector()
        
        # Khá»Ÿi táº¡o há»‡ thá»‘ng
        self._initialize_system()
    
    def _initialize_system(self):
        """Khá»Ÿi táº¡o há»‡ thá»‘ng vá»›i hybrid model"""
        print("ğŸš— Äang khá»Ÿi táº¡o há»‡ thá»‘ng nháº­n dáº¡ng biá»ƒn sá»‘ xe...")
        
        # Load hybrid model (best performance)
        hybrid_model_path = "models/knn_character_recognizer_hybrid.pkl"
        if os.path.exists(hybrid_model_path):
            print("ğŸ¤– Äang táº£i Hybrid KNN model...")
            try:
                with open(hybrid_model_path, 'rb') as f:
                    data = pickle.load(f)
                    self.model = data.get('model', data)
                self.recognition_method = 'hybrid'
                print("âœ… ÄÃ£ táº£i Hybrid model thÃ nh cÃ´ng! (57.81% accuracy)")
            except Exception as e:
                print(f"âŒ Lá»—i táº£i model: {e}")
                sys.exit(1)
        else:
            print(f"âŒ KhÃ´ng tÃ¬m tháº¥y model táº¡i: {hybrid_model_path}")
            print("ğŸ’¡ Vui lÃ²ng cháº¡y: python scripts/train_knn_hybrid.py")
            sys.exit(1)
        
        print("âœ… Há»‡ thá»‘ng Ä‘Ã£ sáºµn sÃ ng!")
    
    def extract_features(self, char_img):
        """Extract features tá»« character image"""
        resized = cv2.resize(char_img, (20, 30))
        _, binary = cv2.threshold(resized, 128, 255, cv2.THRESH_BINARY_INV)
        features = binary.flatten().astype(np.float32) / 255.0
        features = np.clip(features, 0, 1)
        return features
    
    def process_image(self, image_path, visualize=True):
        """
        Pipeline hoÃ n chá»‰nh tá»«ng bÆ°á»›c:
        1. Tiá»n xá»­ lÃ½
        2. PhÃ¡t hiá»‡n biá»ƒn sá»‘
        3. Hiá»‡u chá»‰nh gÃ³c nghiÃªng
        4. Segment kÃ½ tá»±
        5. Nháº­n dáº¡ng kÃ½ tá»±
        """
        print(f"ğŸ” Äang xá»­ lÃ½ áº£nh: {image_path}")
        print("="*60)
        
        # Äá»c áº£nh
        image = cv2.imread(image_path)
        if image is None:
            print(f"âŒ KhÃ´ng thá»ƒ Ä‘á»c áº£nh: {image_path}")
            return None
        
        start_time = time.time()
        h, w = image.shape[:2]
        print(f"ğŸ“ KÃ­ch thÆ°á»›c áº£nh: {w}x{h}")
        
        # STEP 1: Tiá»n xá»­ lÃ½ áº£nh
        print("\n[Step 1] Tiá»n xá»­ lÃ½ áº£nh...")
        preprocessed = self.preprocessor.preprocess(image)
        print("  âœ… ÄÃ£ preprocess (grayscale, blur)")
        
        # STEP 2: PhÃ¡t hiá»‡n biá»ƒn sá»‘
        print("\n[Step 2] PhÃ¡t hiá»‡n biá»ƒn sá»‘...")
        plates = self.plate_detector.detect_plates(preprocessed)
        print(f"  ğŸ¯ PhÃ¡t hiá»‡n: {len(plates)} biá»ƒn sá»‘")
        
        if len(plates) == 0:
            print("  âš ï¸  KhÃ´ng phÃ¡t hiá»‡n biá»ƒn sá»‘ nÃ o")
            return None
        
        results = []
        
        # STEP 3-5: Vá»›i má»—i biá»ƒn sá»‘
        print("\n[Step 3-5] Hiá»‡u chá»‰nh gÃ³c - Segment kÃ½ tá»± - Nháº­n dáº¡ng...")
        for plate_idx, (x, y, w_plate, h_plate) in enumerate(plates, 1):
            # Crop biá»ƒn sá»‘ tá»« áº£nh gá»‘c
            plate_roi = image[y:y+h_plate, x:x+w_plate]
            
            print(f"\n  Biá»ƒn sá»‘ #{plate_idx}: ({x}, {y}) - {w_plate}x{h_plate}")
            
            # Step 3: Hiá»‡u chá»‰nh gÃ³c nghiÃªng
            corrected_roi, skew_angle = self.skew_corrector.correct_skew(plate_roi)
            if abs(skew_angle) > 0.5:
                print(f"    ğŸ”„ Hiá»‡u chá»‰nh gÃ³c: {skew_angle:.1f}Â°")
                plate_roi = corrected_roi
            else:
                print(f"    âœ… GÃ³c Ä‘Ã£ tháº³ng ({skew_angle:.1f}Â°)")
            
            # Step 4: Segment kÃ½ tá»±
            char_images = self.char_recognizer.segment_characters(plate_roi)
            
            if len(char_images) == 0:
                print(f"    âš ï¸  KhÃ´ng segment Ä‘Æ°á»£c kÃ½ tá»±")
                continue
            
            print(f"    ğŸ“‹ Segment: {len(char_images)} kÃ½ tá»±")
            
            # Nháº­n dáº¡ng
            if self.model is not None:
                features_list = [self.extract_features(char) for char in char_images]
                features_array = np.array(features_list)
                predictions = self.model.predict(features_array)
                plate_text = ''.join(predictions)
                print(f"    âœ… Káº¿t quáº£: {plate_text}")
            else:
                plate_text = "N/A"
                print(f"    âš ï¸  Model khÃ´ng sáºµn sÃ ng")
            
            results.append({
                'position': (x, y, w_plate, h_plate),
                'text': plate_text,
                'char_count': len(char_images)
            })
        
        processing_time = time.time() - start_time
        
        # Visualize
        if visualize:
            result_image = image.copy()
            
            # Váº½ biá»ƒn sá»‘ bounding boxes
            for plate_idx, (x, y, w_p, h_p) in enumerate(plates, 1):
                cv2.rectangle(result_image, (x, y), (x + w_p, y + h_p), (0, 255, 0), 2)
                
                # ThÃªm text nháº­n dáº¡ng náº¿u cÃ³
                if plate_idx <= len(results):
                    plate_text = results[plate_idx - 1]['text']
                    cv2.putText(result_image, plate_text, (x, y - 5),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            
            cv2.imshow(f"License Plate Recognition - {Path(image_path).name}", result_image)
            cv2.waitKey(0)
            cv2.destroyAllWindows()
        
        print(f"\nâ±ï¸  Thá»i gian xá»­ lÃ½: {processing_time:.3f}s")
        print("="*60)
        
        return results
    
    def process_video(self, video_path, output_path=None):
        """
        Xá»­ lÃ½ video Ä‘áº§u vÃ o
        """
        print(f"ğŸ¥ Äang xá»­ lÃ½ video: {video_path}")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"âŒ KhÃ´ng thá»ƒ má»Ÿ video: {video_path}")
            return
        
        # Thiáº¿t láº­p output video náº¿u cÃ³
        if output_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            fps = cap.get(cv2.CAP_PROP_FPS)
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        frame_count = 0
        processing_times = []
        results_list = []
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            
            # Xá»­ lÃ½ má»—i 3 frame Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™
            if frame_count % 3 != 0:
                continue
            
            start_time = time.time()
            
            # Segment vÃ  recognize
            char_images = self.char_recognizer.segment_characters(frame)
            
            plate_text = ""
            if len(char_images) > 0:
                features_list = [self.extract_features(char) for char in char_images]
                features_array = np.array(features_list)
                predictions = self.model.predict(features_array)
                plate_text = ''.join(predictions)
                results_list.append({'frame': frame_count, 'plate': plate_text})
            
            processing_time = time.time() - start_time
            processing_times.append(processing_time)
            
            # Váº½ káº¿t quáº£ lÃªn frame
            if plate_text:
                cv2.rectangle(frame, (10, 10), (300, 50), (0, 255, 0), 2)
                cv2.putText(frame, f"Plate: {plate_text}", (20, 40), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
            
            # Hiá»ƒn thá»‹ FPS
            fps = 1.0 / processing_time if processing_time > 0 else 0
            cv2.putText(frame, f"FPS: {fps:.1f}", (frame.shape[1]-150, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            
            # Hiá»ƒn thá»‹ frame
            cv2.imshow('License Plate Recognition', frame)
            
            # Ghi video output
            if output_path:
                out.write(frame)
            
            # ThoÃ¡t náº¿u nháº¥n 'q'
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        # TÃ­nh toÃ¡n thá»‘ng kÃª
        if processing_times:
            avg_fps = 1.0 / (sum(processing_times) / len(processing_times))
            print(f"\nğŸ“Š THá»NG KÃŠ VIDEO:")
            print(f"   - Tá»•ng frame: {frame_count}")
            print(f"   - FPS trung bÃ¬nh: {avg_fps:.1f}")
            print(f"   - Biá»ƒn sá»‘ phÃ¡t hiá»‡n: {len(results_list)}")
            if results_list:
                print(f"   - VÃ­ dá»¥: {results_list[0]['plate']}")
        
        # Giáº£i phÃ³ng tÃ i nguyÃªn
        cap.release()
        if output_path:
            out.release()
        cv2.destroyAllWindows()
    
    def process_batch(self, image_dir, output_csv=None):
        """
        Xá»­ lÃ½ batch áº£nh tá»« folder
        """
        print(f"ğŸ“ Äang xá»­ lÃ½ batch tá»«: {image_dir}")
        
        image_dir = Path(image_dir)
        image_files = sorted(image_dir.glob('*.png')) + sorted(image_dir.glob('*.jpg'))
        
        results = []
        
        for i, img_path in enumerate(image_files, 1):
            print(f"\n[{i}/{len(image_files)}] {img_path.name}")
            
            result = self.process_image(str(img_path), visualize=False)
            
            if result:
                results.append({
                    'filename': img_path.name,
                    'plate_text': result['plate_text'],
                    'char_count': result['char_count'],
                    'processing_time': result['processing_time']
                })
        
        # LÆ°u káº¿t quáº£
        if output_csv:
            df = pd.DataFrame(results)
            df.to_csv(output_csv, index=False)
            print(f"\nğŸ’¾ ÄÃ£ lÆ°u káº¿t quáº£ táº¡i: {output_csv}")
        
        # Thá»‘ng kÃª
        print(f"\nğŸ“Š THá»NG KÃŠ:")
        print(f"   - Tá»•ng áº£nh: {len(image_files)}")
        print(f"   - Xá»­ lÃ½ thÃ nh cÃ´ng: {len(results)}")
        print(f"   - Thá»i gian trung bÃ¬nh: {np.mean([r['processing_time'] for r in results]):.3f}s")
        
        return results
    
    def evaluate_on_annotations(self, image_dir, annotation_csv):
        """
        ÄÃ¡nh giÃ¡ há»‡ thá»‘ng trÃªn dataset vá»›i annotations
        """
        print(f"ğŸ“Š Äang Ä‘Ã¡nh giÃ¡ trÃªn dataset: {image_dir}")
        
        # Load annotations
        df = pd.read_csv(annotation_csv)
        print(f"âœ… Loaded {len(df)} annotations")
        
        results = []
        detected_count = 0
        
        for idx, row in df.iterrows():
            filename = row['filename']
            ground_truth = row['plate_text']
            img_path = f"{image_dir}/{filename}"
            
            if not os.path.exists(img_path):
                continue
            
            img = cv2.imread(img_path)
            if img is None:
                continue
            
            # Get bounding box
            try:
                x1, y1 = int(row['xmin']), int(row['ymin'])
                x2, y2 = int(row['xmax']), int(row['ymax'])
                plate_img = img[y1:y2, x1:x2]
            except:
                continue
            
            # Segment characters
            char_images = self.char_recognizer.segment_characters(plate_img)
            
            if len(char_images) == 0:
                continue
            
            detected_count += 1
            
            # Recognize
            features_list = [self.extract_features(char) for char in char_images]
            features_array = np.array(features_list)
            predictions = self.model.predict(features_array)
            predicted = ''.join(predictions)
            
            # Calculate accuracy
            accuracy = self._calculate_accuracy(predicted, ground_truth)
            
            results.append({
                'filename': filename,
                'ground_truth': ground_truth,
                'predicted': predicted,
                'accuracy': accuracy
            })
            
            status = "âœ…" if accuracy == 100 else "âš ï¸ "
            print(f"{status} {filename:20s} GT: {ground_truth:15s} Pred: {predicted:15s} Acc: {accuracy:6.1f}%")
        
        # Summary
        if results:
            avg_accuracy = sum(r['accuracy'] for r in results) / len(results)
            perfect = sum(1 for r in results if r['accuracy'] == 100)
            partial = sum(1 for r in results if r['accuracy'] > 0)
            
            print(f"\nğŸ“ˆ Káº¾T QUáº¢:")
            print(f"   - PhÃ¡t hiá»‡n kÃ½ tá»±: {detected_count}/{len(df)}")
            print(f"   - Äá»™ chÃ­nh xÃ¡c trung bÃ¬nh: {avg_accuracy:.2f}%")
            print(f"   - Perfect (100%): {perfect}/{len(results)}")
            print(f"   - Partial (>0%): {partial}/{len(results)}")
    
    def _calculate_accuracy(self, predicted, ground_truth):
        """TÃ­nh accuracy kÃ½ tá»±"""
        if len(ground_truth) == 0:
            return 0.0
        correct = sum(1 for p, g in zip(predicted, ground_truth) if p == g)
        return (correct / len(ground_truth)) * 100
    
    def _visualize_result(self, image, plate_text, char_images, predictions):
        """Visualize káº¿t quáº£ lÃªn áº£nh"""
        result_img = image.copy()
        h, w = result_img.shape[:2]
        
        # Add title
        title = f"Recognized Plate: {plate_text}"
        cv2.putText(result_img, title, (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        
        # Draw segmented characters at bottom
        char_display_height = 100
        char_height = h - char_display_height
        char_width = w // len(char_images) if len(char_images) > 0 else w
        
        for i, (char_img, pred) in enumerate(zip(char_images, predictions)):
            x = i * char_width
            # Draw character box
            cv2.rectangle(result_img, (x, char_height), (x + char_width, h), (0, 255, 0), 2)
            # Draw predicted character
            cv2.putText(result_img, pred, (x + 15, h - 20), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 2)
        
        # Show
        cv2.imshow('License Plate Recognition - Result', result_img)
        cv2.waitKey(0)
        cv2.destroyAllWindows()

def main():
    """HÃ m main Ä‘á»ƒ cháº¡y há»‡ thá»‘ng"""
    parser = argparse.ArgumentParser(description='Há»‡ thá»‘ng nháº­n dáº¡ng biá»ƒn sá»‘ xe (Hybrid Model)')
    parser.add_argument('--image', type=str, help='ÄÆ°á»ng dáº«n áº£nh Ä‘áº§u vÃ o')
    parser.add_argument('--batch', type=str, help='Xá»­ lÃ½ batch tá»« folder')
    parser.add_argument('--video', type=str, help='ÄÆ°á»ng dáº«n video Ä‘áº§u vÃ o')
    parser.add_argument('--output', type=str, help='ÄÆ°á»ng dáº«n output')
    parser.add_argument('--eval', type=str, help='Folder Ä‘á»ƒ Ä‘Ã¡nh giÃ¡')
    parser.add_argument('--annotations', type=str, help='File CSV annotations')
    
    args = parser.parse_args()
    
    # Khá»Ÿi táº¡o há»‡ thá»‘ng
    lpr_system = LicensePlateSystem()
    
    # Cháº¡y cháº¿ Ä‘á»™ tÆ°Æ¡ng á»©ng
    if args.image:
        lpr_system.process_image(args.image)
    elif args.batch:
        lpr_system.process_batch(args.batch, args.output)
    elif args.video:
        lpr_system.process_video(args.video, args.output)
    elif args.eval and args.annotations:
        lpr_system.evaluate_on_annotations(args.eval, args.annotations)
    else:
        print("\nğŸ¯ USAGE:")
        print("  # Process single image:")
        print("  python main.py --image path/to/image.jpg")
        print("\n  # Process batch:")
        print("  python main.py --batch path/to/folder --output results.csv")
        print("\n  # Process video:")
        print("  python main.py --video input.mp4 --output output.mp4")
        print("\n  # Evaluate on dataset:")
        print("  python main.py --eval datasets/kaggle_foreign/test --annotations test_annotations.csv")

if __name__ == "__main__":
    main()